{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../models/')\n",
    "sys.path.insert(0,'../datasets/')\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from models import PropbankEncoder\n",
    "import config\n",
    "\n",
    "INPUT_DIR = '../datasets/binaries/'\n",
    "PROPBANK_GLO50_PATH = '{:}deep_glo50.pickle'.format(INPUT_DIR)\n",
    "PEARL_SRLEVAL_PATH = '../srlconll-1.1/bin/srl-eval.pl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Structured Prediction Network CWIS SRL (BR)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>In this notebook we solve the semantic role labeling task using structured predictions networks.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Builds a \"human friendly\" version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P</th>\n",
       "      <th>FORM</th>\n",
       "      <th>ARG</th>\n",
       "      <th>T</th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>CHUNK_START</th>\n",
       "      <th>CHUNK_FINISH</th>\n",
       "      <th>CHUNK_LEN</th>\n",
       "      <th>CHUNK_CANDIDATE_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Brasília</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Pesquisa_Datafolha</td>\n",
       "      <td>(A0*</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>publicada</td>\n",
       "      <td>*</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>hoje</td>\n",
       "      <td>*)</td>\n",
       "      <td>A0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>revela</td>\n",
       "      <td>(V*)</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>um</td>\n",
       "      <td>(A1*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>dado</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>supreendente</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>recusando</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>uma</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>postura</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>radical</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>esmagadora</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>maioria</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>(</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>%</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>)</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>os</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>eleitores</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>quer</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>PT</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>participando</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Governo</td>\n",
       "      <td>*</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>Fernando_Henrique_Cardoso</td>\n",
       "      <td>*)</td>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  P                       FORM   ARG   T  CHUNK_ID  CHUNK_START  \\\n",
       "INDEX                                                                      \n",
       "0       1  1                   Brasília     *   *         1            0   \n",
       "1       2  1         Pesquisa_Datafolha  (A0*  A0         2            1   \n",
       "2       3  1                  publicada     *  A0         2            1   \n",
       "3       4  1                       hoje    *)  A0         2            1   \n",
       "4       5  1                     revela  (V*)   V         3            4   \n",
       "5       6  1                         um  (A1*  A1         4            5   \n",
       "6       7  1                       dado     *  A1         4            5   \n",
       "7       8  1               supreendente     *  A1         4            5   \n",
       "8       9  1                          :     *  A1         4            5   \n",
       "9      10  1                  recusando     *  A1         4            5   \n",
       "10     11  1                        uma     *  A1         4            5   \n",
       "11     12  1                    postura     *  A1         4            5   \n",
       "12     13  1                    radical     *  A1         4            5   \n",
       "13     14  1                          ,     *  A1         4            5   \n",
       "14     15  1                          a     *  A1         4            5   \n",
       "15     16  1                 esmagadora     *  A1         4            5   \n",
       "16     17  1                    maioria     *  A1         4            5   \n",
       "17     18  1                          (     *  A1         4            5   \n",
       "18     19  1                         77     *  A1         4            5   \n",
       "19     20  1                          %     *  A1         4            5   \n",
       "20     21  1                          )     *  A1         4            5   \n",
       "21     22  1                         de     *  A1         4            5   \n",
       "22     23  1                         os     *  A1         4            5   \n",
       "23     24  1                  eleitores     *  A1         4            5   \n",
       "24     25  1                       quer     *  A1         4            5   \n",
       "25     26  1                          o     *  A1         4            5   \n",
       "26     27  1                         PT     *  A1         4            5   \n",
       "27     28  1               participando     *  A1         4            5   \n",
       "28     29  1                         de     *  A1         4            5   \n",
       "29     30  1                          o     *  A1         4            5   \n",
       "30     31  1                    Governo     *  A1         4            5   \n",
       "31     32  1  Fernando_Henrique_Cardoso    *)  A1         4            5   \n",
       "32     33  1                          .     *   *         5           32   \n",
       "\n",
       "       CHUNK_FINISH  CHUNK_LEN  CHUNK_CANDIDATE_ID  \n",
       "INDEX                                               \n",
       "0                 1          1                   0  \n",
       "1                 4          3                  35  \n",
       "2                 4          3                  35  \n",
       "3                 4          3                  35  \n",
       "4                 5          1                 126  \n",
       "5                32         27                 181  \n",
       "6                32         27                 181  \n",
       "7                32         27                 181  \n",
       "8                32         27                 181  \n",
       "9                32         27                 181  \n",
       "10               32         27                 181  \n",
       "11               32         27                 181  \n",
       "12               32         27                 181  \n",
       "13               32         27                 181  \n",
       "14               32         27                 181  \n",
       "15               32         27                 181  \n",
       "16               32         27                 181  \n",
       "17               32         27                 181  \n",
       "18               32         27                 181  \n",
       "19               32         27                 181  \n",
       "20               32         27                 181  \n",
       "21               32         27                 181  \n",
       "22               32         27                 181  \n",
       "23               32         27                 181  \n",
       "24               32         27                 181  \n",
       "25               32         27                 181  \n",
       "26               32         27                 181  \n",
       "27               32         27                 181  \n",
       "28               32         27                 181  \n",
       "29               32         27                 181  \n",
       "30               32         27                 181  \n",
       "31               32         27                 181  \n",
       "32               33          1                 560  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfgs = pd.read_csv('../datasets/csvs/gs.csv', index_col=0, sep=',', encoding='utf-8')\n",
    "column_files = [\n",
    "    '../datasets/csvs/column_chunks/chunks.csv',\n",
    "    '../datasets/csvs/column_predmarker/predicate_marker.csv',\n",
    "    '../datasets/csvs/column_shifts_ctx_p/form.csv',\n",
    "    '../datasets/csvs/column_shifts_ctx_p/gpos.csv',\n",
    "    '../datasets/csvs/column_shifts_ctx_p/lemma.csv',\n",
    "    '../datasets/csvs/column_t/t.csv',\n",
    "    '../datasets/csvs/column_iob/iob.csv'\n",
    "]\n",
    "\n",
    "for col_f in column_files:\n",
    "    _df = pd.read_csv(col_f, index_col=0, encoding='utf-8')\n",
    "    dfgs = pd.concat((dfgs, _df), axis=1)\n",
    "\n",
    "DISPLAY_COLUMNS = ['ID', 'P', 'FORM', 'ARG', 'T', \n",
    "                   'CHUNK_ID', 'CHUNK_START', 'CHUNK_FINISH', 'CHUNK_LEN', 'CHUNK_CANDIDATE_ID']            \n",
    "dfgs[DISPLAY_COLUMNS].head(33)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gets encodings\n",
    "\n",
    "Propbank Encoder holds an indexed version of propbank dataset an answers to FOUR different dataformats: \n",
    "* CAT: this is the raw categorical data.\n",
    "* EMB: tokens are embedding using GloVe embeddings.\n",
    "* HOT: onehot encoding of the words and tokens.\n",
    "* IDX: dense indexed representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD ENCODER\n",
    "propbank_encoder = PropbankEncoder.recover(PROPBANK_GLO50_PATH)\n",
    "db = propbank_encoder.db\n",
    "lex2idx = propbank_encoder.lex2idx\n",
    "idx2lex = propbank_encoder.idx2lex\n",
    "\n",
    "# FOR TEXTUAL DATA ONLY\n",
    "lex2tok = propbank_encoder.lex2tok\n",
    "tok2idx = propbank_encoder.tok2idx\n",
    "embeddings = propbank_encoder.embeddings\n",
    "\n",
    "n_targets = len(lex2idx['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes\t 44 \n",
      " records\t 141730\n"
     ]
    }
   ],
   "source": [
    "print('attributes\\t',\n",
    "       len(db),\n",
    "      '\\n',             \n",
    "      'records\\t',\n",
    "       len(db['ARG'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_type(ds_type, db):\n",
    "    '''Filters only records from train dataset\n",
    "    '''\n",
    "    ds_types = ('train', 'test', 'valid')\n",
    "    if ds_type not in ds_types:\n",
    "        _msg = 'ds_type must be in {:} got {:}'\n",
    "        _msg = _msg.format(ds_types, ds_type)\n",
    "        raise ValueError(_msg)\n",
    "    elif ds_type in ('train',):\n",
    "        lb = 0 \n",
    "        ub = config.DATASET_TRAIN_SIZE\n",
    "    elif ds_type in ('test',):        \n",
    "        lb = config.DATASET_TRAIN_SIZE\n",
    "        ub = lb + config.DATASET_VALID_SIZE         \n",
    "    elif ds_type in ('valid',):                \n",
    "        lb = config.DATASET_TRAIN_SIZE + config.DATASET_VALID_SIZE\n",
    "        ub = lb + config.DATASET_TEST_SIZE         \n",
    "\n",
    "    sel_keys_ = {key_ for key_, prop_ in db['P'].items() if prop_ > lb and prop_ <= ub}\n",
    "\n",
    "    return {\n",
    "                attr_:{ idx_: i_\n",
    "                        for idx_, i_ in dict_.items() if idx_ in sel_keys_\n",
    "                      }        \n",
    "                for attr_, dict_  in db.items()\n",
    "            }\n",
    "\n",
    "def make_propositions_dict(db):\n",
    "    '''Reindex db by propositions creating a nested dict in which the\n",
    "        outer key is the proposition        \n",
    "    '''\n",
    "    \n",
    "    triple_list = []\n",
    "    prev_prop = -1\n",
    "    for idx, prop in db['P'].items():\n",
    "        if prev_prop != prop:\n",
    "            if idx > 0:\n",
    "                ub = idx-1\n",
    "                triple_list.append((lb, ub, prev_prop))\n",
    "            lb = idx\n",
    "        prev_prop = prop\n",
    "    triple_list.append((lb, ub, prev_prop))\n",
    "            \n",
    "\n",
    "        \n",
    "    prop_set = set(db['P'].values())\n",
    "    return { prop_:\n",
    "                    {\n",
    "                        attr_:{ idx_: dict_[idx_]\n",
    "                                for idx_ in range(lb_, ub_ + 1, 1)\n",
    "                          }        \n",
    "                        for attr_, dict_ in db.items() if attr_ not in ('P',)\n",
    "                    }\n",
    "             for lb_, ub_, prop_ in  triple_list\n",
    "            }, {prop_: ub_ - lb_ + 1 for lb_, ub_, prop_ in  triple_list}   \n",
    "\n",
    "\n",
    "def numpfy_propositions_dict(prop_dict, proplen_dict):\n",
    "    '''Converts inner dict examples into numpy arrays\n",
    "    '''\n",
    "    prop_dict_ = defaultdict(dict)    \n",
    "    for prop, columns_dict in prop_dict.items():\n",
    "        len_ = proplen_dict[prop]\n",
    "        shape_ = (len_, 1)\n",
    "        for column, values_dict in columns_dict.items():\n",
    "            tuple_list = [idx_value \n",
    "                          for idx_value in values_dict.items()]\n",
    "            \n",
    "            tuple_list = sorted(tuple_list, key=lambda x: x[0])            \n",
    "            # Converts lexicon (raw/indexed) into token (embedded/indexed)\n",
    "            if (('FORM' in column) or ('LEMMA' in column)):\n",
    "                values_list = [tok2idx[lex2tok[idx2lex[column][tuple_[1]]]]                \n",
    "                                   for tuple_ in tuple_list]\n",
    "            else:\n",
    "                values_list = [tuple_[1] for tuple_ in tuple_list]\n",
    "            \n",
    "            prop_dict_[prop][column]  = np.array(values_list).reshape(shape_)\n",
    "    \n",
    "    return prop_dict_        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes\t 44 \n",
      " records\t 123846 \n",
      " vocab\t 13289\n"
     ]
    }
   ],
   "source": [
    "traindb  = filter_type('train', db)\n",
    "print('attributes\\t',\n",
    "       len(traindb),\n",
    "      '\\n',             \n",
    "      'records\\t',\n",
    "       len(traindb['ARG'].keys()),\n",
    "       '\\n',             \n",
    "      'vocab\\t',\n",
    "        max([form for _, form in traindb['FORM'].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes\t 44 \n",
      " records\t 123837 \n",
      " vocab\t 13289\n"
     ]
    }
   ],
   "source": [
    "prop_dict, proplen_dict = make_propositions_dict(traindb)\n",
    "print('attributes\\t',\n",
    "       len(prop_dict[1]) + 1,\n",
    "      '\\n',             \n",
    "      'records\\t',\n",
    "       sum([len(d['ARG']) for p, d in prop_dict.items()]),\n",
    "        '\\n',             \n",
    "      'vocab\\t',\n",
    "        max([form for _, prop in prop_dict.items() for _, form in prop['FORM'].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes\t 44 \n",
      " records\t 123837 \n",
      " vocab\t 12037\n"
     ]
    }
   ],
   "source": [
    "prop_dict1 = numpfy_propositions_dict(prop_dict, proplen_dict)\n",
    "print('attributes\\t',\n",
    "       len(prop_dict1[1]) + 1,\n",
    "      '\\n',             \n",
    "      'records\\t',\n",
    "       sum([len_ for _, len_ in proplen_dict.items()]),\n",
    "        '\\n',             \n",
    "      'vocab\\t',\n",
    "        max([max(form) for _, prop in prop_dict1.items() for form in prop['FORM']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(db1, propid):\n",
    "    '''Generate inputs\n",
    "    '''\n",
    "    propdb = db1[propid] # nested dict of columns and idx value\n",
    "    if 'CHUNK_SPACE' not in propdb:\n",
    "        proplen = len(propdb['ID'])\n",
    "        propdb['CHUNK_SPACE'] = generate_chunk_space(proplen)\n",
    "\n",
    "    word    = propdb['FORM']\n",
    "    ctx_p_left  = propdb['FORM_CTX_P-1']\n",
    "    ctx_p0  = propdb['FORM_CTX_P+0']\n",
    "    ctx_p_right  = propdb['FORM_CTX_P+1']\n",
    "    \n",
    "    marker  = propdb['MARKER']\n",
    "    pos     = propdb['GPOS']\n",
    "    chunk_type  = propdb['T']\n",
    "    chunk_start, chunk_finish = propdb['CHUNK_SPACE']\n",
    "    \n",
    "    return word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish\n",
    "            \n",
    "def generate_chunk_space(n):\n",
    "    '''Generates all possible spaces for chunks\n",
    "    '''\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    for i in range(n):\n",
    "        for j in range(i,n,1):\n",
    "            start_list.append(i)\n",
    "            end_list.append(j+1)\n",
    "    shape_ = (len(start_list), 1)\n",
    "    start_ = np.array(start_list).reshape(shape_)\n",
    "    finish_ = np.array(end_list).reshape(shape_)\n",
    "    return start_, finish_\n",
    "            \n",
    "\n",
    "def get_outputs(db1, propid, n_targets):\n",
    "    ''' Generate outputs\n",
    "    '''\n",
    "    propdb = db1[propid] # nested dict of columns and idx value\n",
    "    if 'OUTPUTS' not in propdb: \n",
    "        id_type = np.concatenate(\n",
    "            ( propdb['CHUNK_CANDIDATE_ID'], propdb['T']), axis=1\n",
    "        )\n",
    "\n",
    "        id_type = np.unique( id_type, axis=0)\n",
    "        propdb['OUTPUTS'] = id_type[:,0] * n_targets + id_type[:,1]\n",
    "\n",
    "    return propdb['OUTPUTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888 ns ± 11 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "propid = 1120\n",
    "word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1, propid)\n",
    "targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "# worst proposition 1120 size 92!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  1261 20160  4571  6518]\n",
      "[ 0  0  0 ..., 32 32 32]\n",
      "[ 1  1  1 ..., 33 33 33]\n",
      "[(0, 1), (1, 4), (32, 33), (4, 5), (5, 32)]\n"
     ]
    }
   ],
   "source": [
    "propid = 1\n",
    "word, ctx_p_left, ctx_p0, ctx_p_right, marker, pos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1,  propid)\n",
    "targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "# proplen = proplen_dict[propid]\n",
    "# y = y.reshape((proplen,1))\n",
    "print(targets)\n",
    "_start  = np.repeat(chunk_start, n_targets)\n",
    "_finish = np.repeat(chunk_finish, n_targets)\n",
    "print(_start)\n",
    "print(_finish)\n",
    "print(list(zip(_start[targets].flatten(), _finish[targets].flatten())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct_perc.colored_weighted_interval_scheduling as cwis\n",
    "import struct_perc.weighted_interval_scheduling as wis\n",
    "import struct_perc.utils as spu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_size = len(lex2idx['FORM']) + 1\n",
    "# embed_size = 50\n",
    "\n",
    "# n_pos = len(lex2idx['GPOS'])\n",
    "# # n_type = len(lex2idx['T'])\n",
    "# n_classes  = len(lex2idx['T'])\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# # word index and gpos \n",
    "# tf_words = tf.placeholder(tf.int32, shape=(None,1))\n",
    "# tf_pos = tf.placeholder(tf.int32, shape=(None,1))\n",
    "# # t_x_type = tf.placeholder(tf.int32, shape=(None,1))\n",
    "\n",
    "# # índices de inicio de intervalo\n",
    "# tf_s = tf.placeholder(tf.int32, shape=(None,1))\n",
    "# # índices de fim de intervalo\n",
    "# tf_f = tf.placeholder(tf.int32, shape=(None,1))\n",
    "\n",
    "# # replicamos os indicies de inicio e fim para cada classe de chunk possivel\n",
    "# tf_sc = tf.reshape(\n",
    "#       tf.tile(tf_s,  [1, n_classes]), [-1,1])\n",
    "# tf_fc = tf.reshape(\n",
    "#       tf.tile(tf_f,  [1, n_classes]), [-1,1])\n",
    "\n",
    "# # n_features = (embed_size + n_pos + n_type)\n",
    "# n_features = (embed_size + n_pos)\n",
    "# # hidden_features = 300\n",
    "# W_shape = (n_features, n_classes)\n",
    "# EMBS = tf.constant(embeddings)\n",
    "# # tf_token = tf.Variable(initial_value=None, expected_shape=(embed_size,), dtype=tf.float32, trainable=False)\n",
    "\n",
    "# # geramos os paramteros do modelo\n",
    "# with tf.variable_scope(\"model\"):\n",
    "#     W = tf.Variable(\n",
    "#         tf.random_normal(W_shape, 0, 1/np.sqrt(n_features * n_classes), name='W')\n",
    "#     )\n",
    "#     b = tf.Variable(\n",
    "#         tf.random_normal((n_classes,), 0, 1/np.sqrt(n_classes), name='b')\n",
    "#     )\n",
    "    \n",
    "\n",
    "# # tf_token = tf.nn.embedding_lookup(tf_embeddings, id) \n",
    "# # Recuperamos os embeddings de cada palavra\n",
    "# tf_word_features = tf.gather_nd(EMBS, tf_words)\n",
    "\n",
    "# tf_pos_flat = tf.reshape(tf_pos, [-1])\n",
    "# tf_pos_features = tf.one_hot(tf_pos_flat, depth=n_pos)\n",
    "\n",
    "# # t_x_type_flat = tf.reshape(t_x_type,[-1])\n",
    "# # t_type_features = tf.one_hot(t_x_type_flat, depth=n_type)\n",
    "\n",
    "# # X = tf.concat((t_word_features,t_pos_features,t_type_features),axis=1)\n",
    "# tf_tok_features = tf.concat((tf_word_features,tf_pos_features),axis=1)\n",
    "\n",
    "# # a partir das features do intervalo computamos o score\n",
    "# tf_scores = tf.matmul(tf_tok_features, W) + b\n",
    "\n",
    "# tf_pred = tf.argmax(tf_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tok2idx)\n",
    "embed_size = 50\n",
    "\n",
    "n_pos = len(lex2idx['GPOS'])\n",
    "n_classes = len(lex2idx['T'])\n",
    "n_features = embed_size * 4 + 1 + n_pos\n",
    "\n",
    "n_hidden = 100\n",
    "# W_shape = (n_features, n_hidden)\n",
    "W_shape = (n_hidden, n_hidden)\n",
    "b_shape = (1, n_hidden)\n",
    "\n",
    "W_interval_shape = (2 * n_hidden, n_classes)\n",
    "# W_shape = (hidden_features, n_classes)\n",
    "b_interval_shape = (1, n_classes)\n",
    "\n",
    "# word index \n",
    "X_words = tf.placeholder(tf.int64, shape=(None,1), name='word')\n",
    "\n",
    "# predicate context index (left, predicate, right)\n",
    "X_ctx_p_left = tf.placeholder(tf.int64, shape=(None,1), name='ctx_p_left')\n",
    "X_ctx_p = tf.placeholder(tf.int64, shape=(None,1), name='ctx_p0')\n",
    "X_ctx_p_right = tf.placeholder(tf.int64, shape=(None,1), name='ctx_p_right')\n",
    "\n",
    "# POS tagging feature\n",
    "X_pos = tf.placeholder(tf.int64, shape=(None,1), name='gpos')\n",
    "X_marker = tf.cast( tf.placeholder(tf.int64, shape=(None,1), name='marker'), tf.float32 )\n",
    "EMBS = tf.Variable(embeddings, trainable=False)\n",
    "\n",
    "# Embedded representation\n",
    "with tf.variable_scope(\"features\"):\n",
    "    EMBS_words = tf.gather_nd(EMBS, X_words, name='word_features')\n",
    "\n",
    "    EMBS_ctx_pleft = tf.gather_nd(EMBS, X_ctx_p_left, name='EMBS_ctx_pleft')\n",
    "    EMBS_ctx_p0 = tf.gather_nd(EMBS, X_ctx_p, name='EMBS_ctx_p0')\n",
    "    EMBS_ctx_pright = tf.gather_nd(EMBS, X_ctx_p_right, name='EMBS_ctx_pright')\n",
    "\n",
    "    X_pos_flat = tf.reshape(X_pos, [-1], name='gpos_flat')\n",
    "    X_pos_onehot = tf.one_hot(X_pos_flat, depth=n_pos, name='gpos_onehot')\n",
    "\n",
    "    X = tf.concat((EMBS_words, EMBS_ctx_pleft, EMBS_ctx_p0,\n",
    "                   EMBS_ctx_pright, X_pos_onehot, X_marker),\n",
    "                  axis=1, name='X')\n",
    "    X_batch = tf.expand_dims(X, 0)\n",
    "\n",
    "with tf.variable_scope('gru', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "    fw = tf.nn.rnn_cell.GRUCell(num_units=n_hidden / 2)\n",
    "    bw = tf.nn.rnn_cell.GRUCell(num_units=n_hidden / 2)\n",
    "    \n",
    "    Wo = tf.Variable(tf.truncated_normal(W_shape, stddev=1.0 / np.sqrt(n_features * n_hidden)), name='W' )\n",
    "    bo = tf.Variable(tf.zeros(b_shape, dtype=tf.float32), name='b')\n",
    "\n",
    "    hidden_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=fw,\n",
    "        cell_bw=bw,\n",
    "        inputs=X_batch,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    hidden_fw, hidden_bw = hidden_outputs\n",
    "    Ho = tf.concat((tf.squeeze(hidden_fw, axis=0) ,tf.squeeze(hidden_fw, axis=0)),axis=1)\n",
    "\n",
    "    Z = tf.nn.tanh( tf.matmul( Ho, Wo ) + bo, name='hidden_layer' )\n",
    "\n",
    "# Those are the interval parameters\n",
    "with tf.variable_scope(\"interval\"):\n",
    "    W_interval = tf.Variable(tf.random_normal(W_interval_shape, mean=0.0, stddev=1.0 / np.sqrt(1.0 * n_hidden * n_classes)), name='W_interval')\n",
    "    b_interval = tf.Variable(tf.zeros(b_interval_shape, dtype=tf.float32), name='b_interval')\n",
    "    \n",
    "    # begin of interval\n",
    "    IntervalStart = tf.placeholder(tf.int32, shape=(None,1))\n",
    "    # end of interval\n",
    "    IntervalFinish = tf.placeholder(tf.int32, shape=(None,1))\n",
    "\n",
    "# features from intervals\n",
    "IntervalFinishZ = tf.gather_nd(Z, IntervalFinish-1)\n",
    "IntervalStartZ = tf.gather_nd(Z, IntervalStart)\n",
    "\n",
    "IntervalZ = tf.concat((IntervalFinishZ, IntervalStartZ), axis=1)\n",
    "IntervalScores = tf.matmul(IntervalZ, W_interval) + b_interval\n",
    "\n",
    "ScoresFlat = tf.reshape(IntervalScores, (-1,1)) # column array n_classes * ((len + 1 ) * len) / 2 \n",
    "ScoresMean = tf.reduce_mean(ScoresFlat) # scalar\n",
    "ScoresDiff = ScoresFlat - ScoresMean   # centralize data --> mean zero\n",
    "\n",
    "ScoresStd = tf.sqrt(tf.reduce_sum(ScoresDiff * ScoresDiff))\n",
    "ScoresOp = ScoresDiff /( ScoresStd + 1e-8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tensorflow test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 100) (561, 200) (561, 100) (561, 100) (200, 36) (1, 36) (561, 36)\n"
     ]
    }
   ],
   "source": [
    "propid  = 1\n",
    "words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1, propid)\n",
    "targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     out_scores_interval = sess.run(scores_interval, feed_dict={\n",
    "    arg_list = [Z, IntervalZ, IntervalStartZ, IntervalFinishZ, W_interval, b_interval, IntervalScores]\n",
    "#     ZZ, scores, start_scores, finish_scores = sess.run(arg_list, feed_dict={\n",
    "    data_list = sess.run(arg_list, feed_dict={\n",
    "        X_words:words,\n",
    "        X_ctx_p_left: ctx_p_left,\n",
    "        X_ctx_p: ctx_p0,\n",
    "        X_ctx_p_right: ctx_p_right,        \n",
    "        X_marker: marker,                \n",
    "        X_pos:gpos,\n",
    "        IntervalStart: chunk_start,\n",
    "        IntervalFinish: chunk_finish\n",
    "    })\n",
    "    ZZ, scores, start_scores, finish_scores, Wi, bi, inteval_scores = data_list\n",
    "    # flat gives the score for each candidate\n",
    "\n",
    "print(ZZ.shape, scores.shape, start_scores.shape, finish_scores.shape, Wi.shape, bi.shape, inteval_scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 1) (561, 1)\n",
      "(33, 100)\n"
     ]
    }
   ],
   "source": [
    "print(chunk_start.shape, chunk_finish.shape)\n",
    "\n",
    "print(ZZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Must be true True\n",
      "Must be false False\n",
      "Must be true True\n",
      "Must be true True\n"
     ]
    }
   ],
   "source": [
    "start_scores_test = np.array([ZZ[s,:] for s in chunk_start[:,0].tolist()])\n",
    "print(np.allclose(start_scores_test, start_scores))\n",
    "finish_scores_test = np.array([ZZ[s-1,:] for s in chunk_finish[:,0].tolist()])\n",
    "print(np.allclose(finish_scores_test, finish_scores))\n",
    "print('Must be true', np.allclose(start_scores_test[0, :], finish_scores_test[0,:]))\n",
    "print('Must be false', np.allclose(start_scores_test[1, :], finish_scores_test[1,:]))\n",
    "scores_test = np.concatenate((finish_scores_test, start_scores_test), axis=1)\n",
    "print('Must be true', np.allclose(scores_test[0, :], scores[0,:]))\n",
    "print('Must be true', np.allclose(np.matmul(scores_test,  Wi) + bi, inteval_scores))\n",
    "# start_scores_test = np.concatenate(start_scores_list, axis=1)\n",
    "# print(start_scores_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(sess, x_words, x_ctx_p_left, x_ctx_p0, x_ctx_p_right, x_marker, x_pos, x_chunk_start, x_chunk_finish):\n",
    "    scores = sess.run(ScoresOp,feed_dict={\n",
    "        X_words: x_words,\n",
    "        X_ctx_p_left: x_ctx_p_left,\n",
    "        X_ctx_p: x_ctx_p0,\n",
    "        X_ctx_p_right: x_ctx_p_right,        \n",
    "        X_marker: x_marker,                \n",
    "        X_pos:x_pos,\n",
    "        IntervalStart: x_chunk_start,\n",
    "        IntervalFinish: x_chunk_finish\n",
    "    })\n",
    "    # scores is a ((proplen + 1) * (proplen) / 2) * n_classes  \n",
    "    starts = np.repeat(x_chunk_start, n_classes).reshape((-1,1))\n",
    "    ends = np.repeat(x_chunk_finish, n_classes).reshape((-1,1))\n",
    "    \n",
    "    ck_len = len(x_chunk_start)\n",
    "    colors = np.array(list(np.arange(n_classes))*ck_len)\n",
    "\n",
    "    # Finds best allocation given the scores and the chunk_space\n",
    "    r_int = cwis.compute_schedule(starts.flatten(), ends.flatten(), scores, colors) # index of the cadidates of predicted solution\n",
    "    r_ext = list(zip(starts[r_int].flatten(),ends[r_int].flatten(), colors[r_int].flatten())) # from integer to triple\n",
    "    return r_int, r_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20184 20112 20004 19860 19680 19464 19212 18924 18600 18240 17828 17396\n",
      " 16928 16424 15884 15324 14712 14064 13380 12660 11904 11112 10284  9420\n",
      "  8520  7584  6612  5604  4560  3480  2364  1212    24]\n",
      "[(32, 33, 24), (31, 32, 24), (30, 31, 24), (29, 30, 24), (28, 29, 24), (27, 28, 24), (26, 27, 24), (25, 26, 24), (24, 25, 24), (23, 24, 24), (22, 23, 8), (21, 22, 8), (20, 21, 8), (19, 20, 8), (18, 19, 8), (17, 18, 24), (16, 17, 24), (15, 16, 24), (14, 15, 24), (13, 14, 24), (12, 13, 24), (11, 12, 24), (10, 11, 24), (9, 10, 24), (8, 9, 24), (7, 8, 24), (6, 7, 24), (5, 6, 24), (4, 5, 24), (3, 4, 24), (2, 3, 24), (1, 2, 24), (0, 1, 24)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "p, pe = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)\n",
    "print(p)\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_y_int = tf.placeholder(tf.int32, shape=(None,))\n",
    "# t_y_rshp = tf.reshape(t_y_int, (-1,1))\n",
    "# t_margin_int = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "# t_margin_values = tf.ones(tf.shape(t_y_rshp))*t_margin_int\n",
    "# t_margin_scores = tf.scatter_nd(t_y_rshp, -t_margin_values, tf.shape(t_scores_flat))\n",
    "# t_scores_flat_w_margin = t_scores_flat + t_margin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indices of the correct intervals\n",
    "T = tf.placeholder(tf.int32, shape=(None,), name='T')\n",
    "L = tf.placeholder(tf.int32, shape=(), name='L')\n",
    "# I = tf.to_int32(tf.range(L), name='indices')\n",
    "\n",
    "T_flat = tf.reshape(T, (-1,1), name='T_flat') # column array\n",
    "MarginFactor = tf.placeholder(tf.float32)\n",
    "MarginIndex = tf.ones(tf.shape(T_flat)) * MarginFactor\n",
    "MarginScores = tf.scatter_nd(T_flat, -MarginIndex, tf.shape(ScoresOp)) # oppposite of gather_nd\n",
    "ScoresWithMargin = ScoresOp + MarginScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # índices dos intervalos computados pelo Weighted Interval Scheduling\n",
    "# t_p_int = tf.placeholder(tf.int32, shape=(None,))\n",
    "# # índices dos intervalos corretos\n",
    "# # t_y_int = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# # formatamos os indices dos intervalos preditos e corretos\n",
    "# t_p_rshp = tf.reshape(t_p_int,(-1,1))\n",
    "# # t_y_rshp = tf.reshape(t_y_int,(-1,1))\n",
    "\n",
    "# # score dos intervalos preditos\n",
    "# t_scores_int_p = tf.gather_nd(t_scores_flat_w_margin, t_p_rshp)\n",
    "# # score dos intervalos corretos\n",
    "# t_scores_int_y = tf.gather_nd(t_scores_flat_w_margin, t_y_rshp)\n",
    "\n",
    "# ## função de custo do perceptron estruturado\n",
    "# # WIS\n",
    "# t_cost_int = tf.reduce_sum(t_scores_int_p) - tf.reduce_sum(t_scores_int_y)\n",
    "# t_cost = t_cost_int\n",
    "\n",
    "# # # gradiente descendente no custo do perceptron estruturado\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "# # optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "# train = optimizer.minimize(t_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indices containing the predicted labels from Weighted Interval Scheduling\n",
    "Y = tf.placeholder(tf.int32, shape=(None,), name='predictions')\n",
    "Y_flat = tf.reshape(Y, (-1, 1))\n",
    "\n",
    "# score da estrutura predita\n",
    "ScoreY = tf.gather_nd(ScoresWithMargin, Y_flat, name='predicted_score')\n",
    "# score da estrutura correta\n",
    "ScoreT = tf.gather_nd(ScoresWithMargin, T_flat, name='target_score')\n",
    "\n",
    "# função de custo do perceptron estruturado\n",
    "CostOp = tf.reduce_sum(ScoreY) - tf.reduce_sum(ScoreT)\n",
    "\n",
    "# gradiente descendente no custo do perceptron estruturado\n",
    "Optimizer = tf.train.AdamOptimizer(0.001)\n",
    "TrainOp = Optimizer.minimize(CostOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[    0  1261 20160  4571  6518]\n",
      "(33,)\n",
      "[20184 20112 20004 19860 19680 19464 19212 18924 18600 18240 17828 17396\n",
      " 16928 16424 15884 15324 14712 14064 13380 12660 11904 11112 10284  9420\n",
      "  8520  7584  6612  5604  4560  3480  2364  1212    24]\n"
     ]
    }
   ],
   "source": [
    "predictions, _ = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)\n",
    "print(targets.shape)\n",
    "print(targets)\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Testing cost operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727056, 1)\n",
      "0.112375\n",
      "20184\n",
      "0.0623754\n"
     ]
    }
   ],
   "source": [
    "predictions, chunk_ext = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)\n",
    "\n",
    "interval_start = np.repeat(chunk_start, n_classes).reshape((-1,1))\n",
    "interval_finish = np.repeat(chunk_finish, n_classes).reshape((-1,1))\n",
    "\n",
    "scores_, cost_ = sess.run([ScoresOp, CostOp], feed_dict={\n",
    "    X_words: words,\n",
    "    X_ctx_p_left: ctx_p_left,\n",
    "    X_ctx_p: ctx_p0,\n",
    "    X_ctx_p_right: ctx_p_right,        \n",
    "    X_marker: marker,        \n",
    "    X_pos: gpos,\n",
    "    IntervalStart: interval_start,\n",
    "    IntervalFinish: interval_finish,\n",
    "    T: targets.flatten(),\n",
    "    Y: predictions.flatten(),        \n",
    "    L: proplen_dict[propid],\n",
    "    MarginFactor:0.01})\n",
    "\n",
    "\n",
    "print(scores_.shape)\n",
    "print(cost_)\n",
    "print(np.max(predictions))\n",
    "print(np.sum(scores_[predictions]) - np.sum(scores_[targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00194049   -0.000565748\n"
     ]
    }
   ],
   "source": [
    "colors = np.array(list(np.arange(n_classes))*len(chunk_start))\n",
    "r_int = cwis.compute_schedule(interval_start.flatten(), interval_finish.flatten(), scores_, colors) \n",
    "print(np.mean(scores_[r_int]), ' ', np.mean(scores_[targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059546709"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(scores_[predictions].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0028287403"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(scores_[targets].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 1212, 2364, 3480, 4560, 5604, 6612, 7584, 8520, 9420, 10284, 11112, 11904, 12660, 13380, 14064, 14712, 15324, 15884, 16424, 16928, 17396, 17828, 18240, 18600, 18924, 19212, 19464, 19680, 19860, 20004, 20112, 20184]\n",
      "[24, 1212, 2364, 3480, 4560, 5604, 6612, 7584, 8520, 9420, 10284, 11112, 11904, 12660, 13380, 14064, 14712, 15324, 15900, 16440, 16944, 17412, 17844, 18240, 18600, 18924, 19212, 19464, 19680, 19860, 20004, 20112, 20184]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(predictions))\n",
    "print(sorted(r_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55983\n",
      "0.50983\n"
     ]
    }
   ],
   "source": [
    "propid  = 1\n",
    "words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1, propid)\n",
    "targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "\n",
    "starts = np.repeat(chunk_start,n_classes).reshape((-1,1))\n",
    "ends = np.repeat(chunk_finish, n_classes).reshape((-1,1))\n",
    "\n",
    "scores_ = sess.run(ScoresOp, feed_dict={\n",
    "    X_words: words,\n",
    "    X_ctx_p_left: ctx_p_left,\n",
    "    X_ctx_p: ctx_p0,\n",
    "    X_ctx_p_right: ctx_p_right,        \n",
    "    X_marker: marker,        \n",
    "    X_pos: gpos,\n",
    "    IntervalStart: chunk_start,\n",
    "    IntervalFinish: chunk_finish,\n",
    "    T: targets.flatten(),\n",
    "    Y: predictions.flatten(),        \n",
    "    MarginFactor:0.01\n",
    "})\n",
    "\n",
    "colors = np.repeat(np.arange(n_classes), len(chunk_start))\n",
    "predictions = cwis.compute_schedule(starts.flatten(), ends.flatten(), scores_, colors) \n",
    "\n",
    "cost_ = sess.run(CostOp, feed_dict={\n",
    "    X_words: words,\n",
    "    X_ctx_p_left: ctx_p_left,\n",
    "    X_ctx_p: ctx_p0,\n",
    "    X_ctx_p_right: ctx_p_right,        \n",
    "    X_marker: marker,        \n",
    "    X_pos: gpos,\n",
    "    IntervalStart: chunk_start,\n",
    "    IntervalFinish: chunk_finish,\n",
    "    T:targets.flatten(),\n",
    "    Y:predictions.flatten(),\n",
    "    MarginFactor:0.01\n",
    "})\n",
    "\n",
    "print(cost_)\n",
    "print(np.sum(scores_[predictions]) - np.sum(scores_[targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20196, 1)\n"
     ]
    }
   ],
   "source": [
    "print(scores_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Single Proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0   0.580276\n",
      "0.0   0.258573\n",
      "0.08571428571428572   0.145245\n",
      "0.02702702702702703   0.139344\n",
      "0.02702702702702703   0.0995918\n",
      "0.05   0.0651398\n",
      "0.17647058823529413   0.0280508\n",
      "0.6666666666666666   0.0100175\n",
      "1.0   9.31323e-10  learnt at epoch  177\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "propid = 1\n",
    "words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1, propid)\n",
    "targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "\n",
    "starts = np.repeat(chunk_start,n_classes).reshape((-1,1))\n",
    "ends = np.repeat(chunk_finish, n_classes).reshape((-1,1))\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    scores_ = sess.run(ScoresOp, feed_dict={\n",
    "        X_words: words,\n",
    "        X_ctx_p_left: ctx_p_left,\n",
    "        X_ctx_p: ctx_p0,\n",
    "        X_ctx_p_right: ctx_p_right,        \n",
    "        X_marker: marker,        \n",
    "        X_pos: gpos,\n",
    "        IntervalStart: chunk_start,\n",
    "        IntervalFinish: chunk_finish,\n",
    "        T: targets.flatten(),\n",
    "        Y: predictions.flatten(),        \n",
    "        MarginFactor:0.01\n",
    "    })\n",
    "    \n",
    "    predictions, chunk_ext = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)    \n",
    "    cost_ = sess.run(CostOp, feed_dict={\n",
    "        X_words: words,\n",
    "        X_ctx_p_left: ctx_p_left,\n",
    "        X_ctx_p: ctx_p0,\n",
    "        X_ctx_p_right: ctx_p_right,        \n",
    "        X_marker: marker,        \n",
    "        X_pos: gpos,\n",
    "        IntervalStart: chunk_start,\n",
    "        IntervalFinish: chunk_finish,\n",
    "        T: targets.flatten(),\n",
    "        Y: predictions.flatten(),        \n",
    "        MarginFactor:0.01\n",
    "    })\n",
    "    \n",
    "    predictions_score = np.sum(scores_[predictions])\n",
    "    targets_score = np.sum(scores_[targets])\n",
    "        \n",
    "    sess.run(TrainOp, feed_dict={\n",
    "        X_words: words,\n",
    "        X_ctx_p_left: ctx_p_left,\n",
    "        X_ctx_p: ctx_p0,\n",
    "        X_ctx_p_right: ctx_p_right,        \n",
    "        X_marker: marker,        \n",
    "        X_pos: gpos,\n",
    "        IntervalStart: chunk_start,\n",
    "        IntervalFinish: chunk_finish,\n",
    "        T: targets.flatten(),\n",
    "        Y: predictions.flatten(),        \n",
    "        MarginFactor:0.01})\n",
    "    \n",
    "    colors = np.repeat(np.arange(n_classes), len(chunk_start))\n",
    "    predictions = cwis.compute_schedule(starts.flatten(), ends.flatten(), scores_, colors) \n",
    "\n",
    "    targets_set = set(targets.flatten())\n",
    "    predictions_set = set(predictions.flatten())\n",
    "    yp_common = targets_set.intersection(predictions_set)\n",
    "    yp_total = targets_set.union(predictions_set)\n",
    "    \n",
    "    acc_int = len(yp_common)/len(yp_total)\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print(acc_int, ' ', cost_)\n",
    "    \n",
    "    if cost_ < 0:\n",
    "        break\n",
    "\n",
    "    if acc_int == 1:\n",
    "        print(acc_int, ' ', cost_, ' learnt at epoch ', i)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32, 33, 35), (5, 32, 11), (4, 5, 8), (1, 4, 2), (0, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(starts[predictions].flatten(), ends[predictions].flatten(), colors[predictions])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ConLL evaluation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_to_conll(sess, prop_dict, propid, idx2lex):\n",
    "    gold_list = []\n",
    "    eval_list = []\n",
    "        \n",
    "    words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict, propid)\n",
    "    targets = get_outputs(prop_dict, propid, n_targets)\n",
    "\n",
    "    predictions, chunk_ext = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)    \n",
    "\n",
    "\n",
    "    n_words = len(words)\n",
    "    pred_array = prop_dict[propid]['PRED']\n",
    "    pred_array = pred_array.flatten()\n",
    "    \n",
    "    arg_array = prop_dict[propid]['ARG']\n",
    "    arg_array = arg_array.flatten()\n",
    "    \n",
    "    pred_list = [idx2lex['PRED'][i] for i in pred_array.tolist()]\n",
    "    gold_list_ = [idx2lex['ARG'][i] for i in arg_array.tolist()]\n",
    "    \n",
    "    gold_list += list(zip(pred_list, gold_list_))\n",
    "\n",
    "    arg_list_ = []\n",
    "    for triple_ in sorted(chunk_ext, key= lambda x: x[0]):\n",
    "        lb, ub, arg = triple_            \n",
    "        chunk_list_ = [idx2lex['T'][arg] if i == lb else '*' for i in range(lb, ub)] \n",
    "        if idx2lex['T'][arg] != '*':\n",
    "            chunk_list_[0] = '({:}*'.format(chunk_list_[0])\n",
    "            chunk_list_[-1] = '{:})'.format(chunk_list_[-1])\n",
    "\n",
    "        arg_list_ += chunk_list_\n",
    "        \n",
    "    eval_list += list(zip(pred_list, arg_list_))\n",
    "    eval_list.append(None)\n",
    "    gold_list.append(None)\n",
    "    return gold_list, eval_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testamos o modelo na frase usada para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\t*\t-\t*\n",
      "-\t(A0*\t-\t(A0*\n",
      "-\t*\t-\t*\n",
      "-\t*)\t-\t*)\n",
      "revelar\t(V*)\trevelar\t(V*)\n",
      "-\t(A1*\t-\t(A1*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*\t-\t*\n",
      "-\t*)\t-\t*)\n",
      "-\t*\t-\t*\n",
      "{'*': 0, 'A0': 1, 'A1': 2, 'A2': 3, 'A3': 4, 'A4': 5, 'A5': 6, 'AM-ADV': 7, 'AM-CAU': 8, 'AM-DIR': 9, 'AM-DIS': 10, 'AM-EXT': 11, 'AM-LOC': 12, 'AM-MED': 13, 'AM-MNR': 14, 'AM-NEG': 15, 'AM-PNC': 16, 'AM-PRD': 17, 'AM-REC': 18, 'AM-TMP': 19, 'C-A0': 20, 'C-A1': 21, 'C-A2': 22, 'C-A3': 23, 'C-AM-ADV': 24, 'C-AM-CAU': 25, 'C-AM-DIS': 26, 'C-AM-EXT': 27, 'C-AM-LOC': 28, 'C-AM-MNR': 29, 'C-AM-NEG': 30, 'C-AM-PNC': 31, 'C-AM-PRD': 32, 'C-AM-TMP': 33, 'C-V': 34, 'V': 35}\n"
     ]
    }
   ],
   "source": [
    "propid = 1\n",
    "gold_list, eval_list = tag_to_conll(sess, prop_dict1, propid, idx2lex)\n",
    "for i in range(proplen_dict[propid]):\n",
    "    if gold_list[i] and eval_list[i]:\n",
    "        print('{:}\\t{:}\\t{:}\\t{:}'.format(*gold_list[i], *eval_list[i]))\n",
    "    else:\n",
    "        print('\\n')\n",
    "    \n",
    "\n",
    "print(lex2idx['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(gold_list, eval_list, verbose=True):\n",
    "    gold_path = 'train_gold.conll'    \n",
    "    eval_path = 'train_eval.conll'\n",
    "\n",
    "    with open(gold_path, mode='w') as f:        \n",
    "        for tuple_ in gold_list:\n",
    "            if tuple_ is None:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write('{:}\\t{:}\\n'.format(*tuple_))\n",
    "\n",
    "    with open(eval_path, mode='w') as f:        \n",
    "        for tuple_ in eval_list:\n",
    "            if tuple_ is None:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write('{:}\\t{:}\\n'.format(*tuple_))\n",
    "\n",
    "    pipe = Popen(['perl',PEARL_SRLEVAL_PATH, gold_path, eval_path], stdout=PIPE, stderr=PIPE)\n",
    "\n",
    "    txt, err = pipe.communicate()\n",
    "    txt = txt.decode('UTF-8')\n",
    "    err = err.decode('UTF-8')\n",
    "    \n",
    "    if verbose:\n",
    "        print(txt)\n",
    "\n",
    "    float_list = re.findall(r'(\\d+.\\d+)', txt)\n",
    "    f1 = float(float_list[3]) if len(float_list) > 3 else -1 \n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences    :           1\n",
      "Number of Propositions :           1\n",
      "Percentage of perfect props : 100.00\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall        2       0       0   100.00  100.00  100.00\n",
      "----------\n",
      "        A0        1       0       0   100.00  100.00  100.00\n",
      "        A1        1       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "         V        1       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "\n",
      "f1_score:  100.0\n"
     ]
    }
   ],
   "source": [
    "propid = 1\n",
    "gold_list, eval_list = tag_to_conll(sess, prop_dict1, propid, idx2lex)\n",
    "f1_score = evaluate(gold_list, eval_list, verbose=True)\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "Number of Sentences    :           1\n",
      "Number of Propositions :           1\n",
      "Percentage of perfect props : 100.00\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall        2       0       0   100.00  100.00  100.00\n",
      "----------\n",
      "        A0        1       0       0   100.00  100.00  100.00\n",
      "        A1        1       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "         V        1       0       0   100.00  100.00  100.00\n",
      "------------------------------------------------------------\n",
      "\n",
      "tempo para avaliar:  0.6595838069915771 s\n",
      "f1_score:  100.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(1111)\n",
    "gold_list, eval_list = tag_to_conll(sess, prop_dict1, propid, idx2lex)\n",
    "f1_score = evaluate(gold_list, eval_list, verbose=True)\n",
    "end = time.time()\n",
    "print('tempo para avaliar: ', (end-start), 's')\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \tepoch: 0  \tacc: 0.9375 \tcost: 0.568005\n",
      "Iteration: 25 \tepoch: 0  \tacc: 0.95211786372 \tcost: 0.170991\n",
      "Epoch: 0  \tf1_score: 0.0  \tbest_score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50 \tepoch: 1  \tacc: 0.96 \tcost: 0.18601\n",
      "Iteration: 75 \tepoch: 1  \tacc: 0.953068592058 \tcost: 0.175636\n",
      "Number of Sentences    :          50\n",
      "Number of Propositions :          50\n",
      "Percentage of perfect props :   0.00\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall        4     606     121     0.66    3.20    1.09\n",
      "----------\n",
      "        A0        1      14      27     6.67    3.57    4.65\n",
      "        A1        2      93      45     2.11    4.26    2.82\n",
      "        A2        1      45      11     2.17    8.33    3.45\n",
      "        A3        0       3       2     0.00    0.00    0.00\n",
      "        A4        0     100       2     0.00    0.00    0.00\n",
      "        A5        0       9       0     0.00    0.00    0.00\n",
      "    AM-ADV        0      32       4     0.00    0.00    0.00\n",
      "    AM-CAU        0       4       4     0.00    0.00    0.00\n",
      "    AM-DIR        0      19       0     0.00    0.00    0.00\n",
      "    AM-DIS        0      43       3     0.00    0.00    0.00\n",
      "    AM-EXT        0      53       0     0.00    0.00    0.00\n",
      "    AM-LOC        0       0       7     0.00    0.00    0.00\n",
      "    AM-MNR        0      52       1     0.00    0.00    0.00\n",
      "    AM-NEG        0      50       5     0.00    0.00    0.00\n",
      "    AM-PNC        0      19       0     0.00    0.00    0.00\n",
      "    AM-PRD        0       4       1     0.00    0.00    0.00\n",
      "    AM-REC        0      24       0     0.00    0.00    0.00\n",
      "    AM-TMP        0      42       9     0.00    0.00    0.00\n",
      "------------------------------------------------------------\n",
      "         V        6     156      44     3.70   12.00    5.66\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch: 1  \tf1_score: 0.66  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 \tepoch: 2  \tacc: 0.973684210526 \tcost: 0.147291\n",
      "Iteration: 125 \tepoch: 2  \tacc: 0.95600676819 \tcost: 0.197758\n",
      "Epoch: 2  \tf1_score: 0.14  \tbest_score: 0.66\n",
      "Iteration: 150 \tepoch: 3  \tacc: 0.923076923077 \tcost: 0.101777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 175 \tepoch: 3  \tacc: 0.953153153153 \tcost: 0.0933091\n",
      "Epoch: 3  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 \tepoch: 4  \tacc: 0.96875 \tcost: 0.121876\n",
      "Iteration: 225 \tepoch: 4  \tacc: 0.952641165756 \tcost: 0.0879981\n",
      "Epoch: 4  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 250 \tepoch: 5  \tacc: 0.964285714286 \tcost: 0.105846\n",
      "Iteration: 275 \tepoch: 5  \tacc: 0.949317738791 \tcost: 0.0898303\n",
      "Epoch: 5  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300 \tepoch: 6  \tacc: 0.947368421053 \tcost: 0.102984\n",
      "Iteration: 325 \tepoch: 6  \tacc: 0.939110070258 \tcost: 0.0711961\n",
      "Epoch: 6  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 350 \tepoch: 7  \tacc: 0.96 \tcost: 0.108778\n",
      "Iteration: 375 \tepoch: 7  \tacc: 0.937649880096 \tcost: 0.0682027\n",
      "Epoch: 7  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 400 \tepoch: 8  \tacc: 0.96875 \tcost: 0.0977199\n",
      "Iteration: 425 \tepoch: 8  \tacc: 0.950664136622 \tcost: 0.0759887\n",
      "Epoch: 8  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 450 \tepoch: 9  \tacc: 0.833333333333 \tcost: 0.0471078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 475 \tepoch: 9  \tacc: 0.928767123288 \tcost: 0.0673258\n",
      "Epoch: 9  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 \tepoch: 10  \tacc: 0.933333333333 \tcost: 0.0487361\n",
      "Iteration: 525 \tepoch: 10  \tacc: 0.889273356401 \tcost: 0.0389142\n",
      "Epoch: 10  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 550 \tepoch: 11  \tacc: 0.333333333333 \tcost: 0.0192447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 575 \tepoch: 11  \tacc: 0.890675241158 \tcost: 0.0485354\n",
      "Epoch: 11  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600 \tepoch: 12  \tacc: 0.972972972973 \tcost: 0.125892\n",
      "Iteration: 625 \tepoch: 12  \tacc: 0.732394366197 \tcost: 0.0571445\n",
      "Epoch: 12  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 650 \tepoch: 13  \tacc: -4.0 \tcost: 0.0558225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 675 \tepoch: 13  \tacc: 0.715976331361 \tcost: 0.0618007\n",
      "Epoch: 13  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 700 \tepoch: 14  \tacc: -4.0 \tcost: 0.0562314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 725 \tepoch: 14  \tacc: 0.722826086957 \tcost: 0.0412992\n",
      "Epoch: 14  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 750 \tepoch: 15  \tacc: 0.952380952381 \tcost: 0.0877968\n",
      "Iteration: 775 \tepoch: 15  \tacc: 0.766393442623 \tcost: 0.044323\n",
      "Epoch: 15  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 800 \tepoch: 16  \tacc: 0.833333333333 \tcost: 0.0615832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 825 \tepoch: 16  \tacc: 0.738095238095 \tcost: 0.0750299\n",
      "Epoch: 16  \tf1_score: -1  \tbest_score: 0.66\n",
      "Iteration: 850 \tepoch: 17  \tacc: -4.0 \tcost: 0.0518702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 875 \tepoch: 17  \tacc: 0.658536585366 \tcost: 0.0457314\n",
      "Epoch: 17  \tf1_score: -1  \tbest_score: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Varela/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 900 \tepoch: 18  \tacc: 0.5 \tcost: 0.0529566\n",
      "Iteration: 925 \tepoch: 18  \tacc: 0.716577540107 \tcost: 0.0415998\n",
      "Number of Sentences    :          50\n",
      "Number of Propositions :          50\n",
      "Percentage of perfect props :   0.00\n",
      "\n",
      "              corr.  excess  missed    prec.    rec.      F1\n",
      "------------------------------------------------------------\n",
      "   Overall        5     102     120     4.67    4.00    4.31\n",
      "----------\n",
      "        A0        0      13      28     0.00    0.00    0.00\n",
      "        A1        3      42      44     6.67    6.38    6.52\n",
      "        A2        0      10      12     0.00    0.00    0.00\n",
      "        A3        0       6       2     0.00    0.00    0.00\n",
      "        A4        0       0       2     0.00    0.00    0.00\n",
      "    AM-ADV        0       2       4     0.00    0.00    0.00\n",
      "    AM-CAU        0       3       4     0.00    0.00    0.00\n",
      "    AM-DIS        1       2       2    33.33   33.33   33.33\n",
      "    AM-EXT        0       7       0     0.00    0.00    0.00\n",
      "    AM-LOC        1       3       6    25.00   14.29   18.18\n",
      "    AM-MNR        0       0       1     0.00    0.00    0.00\n",
      "    AM-NEG        0       8       5     0.00    0.00    0.00\n",
      "    AM-PRD        0       1       1     0.00    0.00    0.00\n",
      "    AM-REC        0       5       0     0.00    0.00    0.00\n",
      "    AM-TMP        0       0       9     0.00    0.00    0.00\n",
      "------------------------------------------------------------\n",
      "         V        3      51      47     5.56    6.00    5.77\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 1000\n",
    "# indices = np.arange(config.DATASET_TRAIN_SIZE)\n",
    "indices = np.arange(50) + 1\n",
    "\n",
    "best_score = 0\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for j in range(epochs):\n",
    "    np.random.shuffle(indices)\n",
    "    total_err = 0\n",
    "    total_size = 0\n",
    "    gold_list = []\n",
    "    eval_list = [] \n",
    "    for i, propid in enumerate(indices):     \n",
    "        try: \n",
    "            words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_type, chunk_start, chunk_finish = get_inputs(prop_dict1, propid)\n",
    "            targets = get_outputs(prop_dict1, propid, n_targets)\n",
    "        except KeyError:\n",
    "            print(propid)\n",
    "\n",
    "        predictions, chunk_ext = pred(sess, words, ctx_p_left, ctx_p0, ctx_p_right, marker, gpos, chunk_start, chunk_finish)\n",
    "\n",
    "        _, cost = sess.run([TrainOp, CostOp], feed_dict={\n",
    "            X_words: words,\n",
    "            X_ctx_p_left: ctx_p_left,\n",
    "            X_ctx_p: ctx_p0,\n",
    "            X_ctx_p_right: ctx_p_right,        \n",
    "            X_marker: marker,        \n",
    "            X_pos: gpos,\n",
    "            IntervalStart: chunk_start,\n",
    "            IntervalFinish: chunk_finish,\n",
    "            T: targets.flatten(),\n",
    "            Y: predictions.flatten(),        \n",
    "            MarginFactor:0.01})\n",
    "\n",
    "        total_err += len(set(predictions.tolist()) targets.tolist())\n",
    "        total_size += predictions.shape[0]\n",
    "        \n",
    "        gold_list_, eval_list_ = tag_to_conll(sess, prop_dict1, propid, idx2lex)\n",
    "\n",
    "        gold_list += gold_list_\n",
    "        eval_list += eval_list_\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print('Iteration:', i + j*len(indices) ,'\\tepoch:', j, ' \\tacc:', 1 - total_err/total_size, '\\tcost:', cost)\n",
    "\n",
    "        \n",
    "    f1_score = evaluate(gold_list, eval_list, verbose=False)    \n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        f1_score = evaluate(gold_list, eval_list, verbose=True)    \n",
    "        save_path = saver.save(sess, \"/tmp/model_spn-pt.ckpt\")\n",
    "        \n",
    "    if best_score > .95:\n",
    "        print('best_score:')\n",
    "        break\n",
    "    print('Epoch:', j, ' \\tf1_score:', f1_score, ' \\tbest_score:', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3240, 2667, 2278, 1990,  144])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 180 2319 2771 2991 3240]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
